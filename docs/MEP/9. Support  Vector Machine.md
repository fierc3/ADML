# Support Vector Machine
Decision Bounderis with kernel which are intensiv for outliers. 
Distance should be maximized

Classificationproblems, depending on the Random seed, can have diffrent Decision Boundaries


## Uncertainity in Data
- small difference when making measurements (condition, machine dependent etc.)
- this small diffrence could cause wrong classification when using linear classificaiton
- "sensitive" to outliers
- ![[Pasted image 20220128171014.png]]


## Large Margin Classifier
- not the same issues with _Large Margin Classifier_
- optimized to have a large margin between line and classification for both classes


## Support Vectors
- points which are closest to the line
- Margin is symbolic line whcih is parallel of t he Decision Boundary and goes through the support vectors
- insensitive to outliers
- Only when a support vector changes the decision boundary changes


## Scalar Product
- The scalar / dot product is a numbner.
- Vector mulitiplied with eachother and summed

### Scalar Product and Vector Length
- L^2 norm aka eucledian distance, is the squareRoot of the components.
	- $\|\mathbf{x}\|=\sqrt{x_{1}^{2}+\ldots+x_{n}^{2}}$
- To normalize -> divide vector by its length -> einheitsvektor / unitvector

### Vector triangles
- vectors have length and a direction, but no position

### Cosine Formula
- when vector x and y arent null, then the cosine formula can be used
- ![[Pasted image 20220128172006.png]]


### Scalar Product as Projection
- when y is a unitvecotr the scalar proudct of x . y is the Projection of x on y
- ![[Pasted image 20220128172142.png]]

### Perpendicularity Tests
Scalarproduct of perpendicular vectors is null. Can be used to find perpendicular vector of another vector




## Hyperplanes
A hyperplane is defined by
- an entrypoint / auffahrtspunkt P
- direction vector / richtungsvektor w
	- w has to be a Unit vector / Einheitsvektor 
	- w must be perpendicular to the plane

### Hessian Normal Form
![[Pasted image 20220128172540.png]]

### Shifting Coordinate System
Since vectors dont have a position they cant be moved, the whole system is moved


### SIgned Distances
- hyperplane halfs a room in to a positive and negative halfplane +HP -HP
- w always show in the director of __-HP__!
	- Point in -HP have negative distance
	- Points in +HP have poisitive distance
- ![[Pasted image 20220128172849.png]]



## Large Margin Classifier
- in SVM we label with -1 and +1 (unlike with binary where its 0 and 1)
	- just a naming convention but helps with formula
- Hyperplane splits both classes
- to control the distance of the margin we use M = 1 and just scale it


### Controlling the Marign
- multiple the redline with a constant -> changes sice of the Margin

### Elegant Problem Formulation
- Since we have 2 classes
	- each class can be represented by +1 and -1
	- The +1 - 1 labels can be used in the formula
	- leads to a general constraint
![[Pasted image 20220128173434.png]]


### How big is the margin
to calculate the distance we can read -b from the hessian Normalform. But since we scaled the equation it doenst exactly mathc with the hessian normal form.
To fix this --> normalize

--> $\frac{-(b-1)}{\|\vec{w}\|}+\frac{(b+1)}{\|\vec{w}\|}=\frac{2}{\|\vec{w}\|}$


### Primal Optimization Problem
Goal
- Data points have to be classified correctly
- margin must be maximized


instead of maximizing $\frac{2}{\|w\|}$ we can just minimize $\|w\|$ 
- minimizing of constraints

THis approach can lead to _overfitting_ since we're trying to find a _Line_
---> _hard margin classifier_


## Soft Margin Classifier
- problem with hard magin classifier
	- everypoint has to be set to a class
	- model becomes overfitted
- soft margin allows point which cross the line without switching class

### Outlier Sensitivity in Hard Margin Classifier

![[Pasted image 20220128174158.png]]
- Because of hard margin the point closer to red is still blue


### Outliers in Soft Margin Classifier 
- allows "missclassification" of trainingsdata
- trade-oiff between generalization for unseenData and classificationerrors in training data.
- ![[Pasted image 20220128174341.png]]


### slack variables $\varepsilon$
- $\varepsilon$ = how much can the point be from the margin
- Hyperparameter tuning of C allows the controll how it reacts to margin. Is closer to the margin bad or are "missclassifcations" allowed in the training data
- C should be validated with _Cross-Validation_
- ![[Pasted image 20220128174651.png]]


### Regularization Parameter
C can be used to avoid overfitting
- the smaller C, the softer is the margin -> bigger margin
- the bigger C, the harder -> smaller margin
- C = inf --> Hard Margin Classifier

- squared Opitimization problem with a minmum. 
- Trade of f betwenn training error and margin controller via Hyperparameter tuning of C
	- use cross validation to find best C



## Kernel
Transforming data into higher dimension
- helpful when
	- data isn't linearly seperable
		- sometimes seperatable in higher dimension
		- projection by squaring of data

### What is a kernel?
- calcualtes similiarity values of 2 datapoints  projected in a higher dimension.
- implies a mapping into higher dimensions
- only hypethic -> gives value without executing the projection
- used in
	- linear kernels
	- RBF kernels
	- Sigmoid kernels


#### Linear Kernal
Special form of Polynomial Kernel, but with regular Scalarproduct

#### Polynomial Kernel
- calculates out of 2 datapoint from a lower dimension the scalarprodukt and adds constants R and squares everything with exponent 5
	- $(\vec{x} \cdot \vec{y}+r)^{d}$
	- r = Coefficent
	- d = Degree of the polynom
	- output = scalarproduct of 2 coordinates

#### RBF Kernel
- Radial basis function (RBF) or Gaussian Kernel
	- Projection in infinitf dimensional space
	- similar like a weighted _Nearest Neighbor Model_
	- The closer to a training data point, the more influence it has on the classifciation
- $\exp \left(-\gamma *\|\vec{x}-\vec{y}\|^{2}\right)$
- Gamme is equivalent totthe exponetial Decay of the scaled Euclidiean Distance between vector x and y


## SVM with Kernel
Difficult to use in the optimization formula.
A possible way is with the _Lagrange Method_

### Lagrange Method
........ forfeited for now.....depending on the exercises




## Summary
- SVM efficient with classificaiton -> need to calculate only 1 scalarproduct
- works with Outliers
- Kernel project datpoints in a higher dimensional space








