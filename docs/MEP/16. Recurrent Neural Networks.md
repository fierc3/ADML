# Recurrent Neural Networks
Allows processing of Timelines like the natural language


## Unsupervisd NLP
_Semantik, Word Embedding and Usecases_

### Form Syntax to Smeantics
- syntax = compares grammar between words
- similarity measured with Levenstein-Distance
	- gives pointt for how many steps have to made to transform to another
- semantics = explains meaning of word
	- grammarcheck + search engines use this

### Word Relatedness
- words have relationships when in same _Document_
- measure if 2 words have a releationshuip
- words categorized as related when they discuss the same topic
- TF-IDF --> based on words in wikipedia article

### Word Similarity
- TF-IDF vvery slow --> uses x-million dimensional space
- _Word SImilarity_ uses Context of the words
	- Words before and after are measured 
- Used to predict missing words (ex: obscured letters etc.)


### Continous Bag of Words applied to 3-Grams
- CBoW
- Weight sharing --> neighbours left and right have same


### Mathematic with Text - Word Embeddings
- semantic calculations of word replacemnts
- since they are vectors, they can be used to calculate
- Works kind of
	- King - Man + Wwomen = QUeen
	- Paris - France + Poland = Warschau
	- Programmer - Man + WOman = Homemaker (yikes)


## Supervised NLP

Timelineanalysis on Textdata:

--> Sequence-To-Vector Models
Sentiment Analyis; Input is a Sequence of fix sized vector --> positive or negative annotation

--> Vector-To-Sequence Models
Image Captioning, Input fixed-size vectorm Output a sequence

--> Sequence-To-Sequence Models
Text Summarization, translations; input and output are sequences

Vanilla NN and CNNs always expetc same size In- and Output fÃ¼r constant sizes


### Recurrent Neural Networks (RNN)
- can be use for any kind of timeseries data
- hiddenstates influence calculation of next hiddenstate
- Outputs always dependant of predecessor


### Deep Inside a Sequence-To-Sequence RNN
- uses ssame weight and bias for every step. h_0 initialized with a null-vector

#### Vanishing Gradient Problems of RNNs
- Since its recurrent the gradient will become smaller and disapper.
	- also possible that it explodes
- Problem also with long term memory
	- When predicting a sentecnce, it has to remember the Subject from the beginning


### Gated Recurrent Units  (GRU)
- solves vanishing gradient problem
- doesn't calculate h_t but how much it is different from the changed value
- Adding $\sigmoid$ Sigmoid multiplaction the network can differentiate between the if the passed value should be use or not.
- FOr new hidden states the _tanh_ activationfunction is used

 ### Bidirectional RNNs
- Can see the "Future"
- need to know what we will write after


### Encoder-Decoder Architecture
- RNN can only be used when  Input and Output have same LENGTH
- if not --> encoder-decoder architecture is needed


#### Unidirectional Encoder-Decoder Architecture
- Rnn are connected together
	- Output of first is input of second
	- Hidden layers can be anything
- Input is encoded und used as fixed size contect for decoding
- Every output of yi ist input of yi+1.
	- Input y1 is the context vector + start token


## Attention Models
- remembers where relevant information of previous words can be used
- mimic human procedure
- Deep RNN --> more complex structures it can learn
	- increase numbers of layers in encoder
		- 1x bidirectional RNN with unidirectional RNN



### RNNs do not parallelize
- doesnt need to remember everything but knows where to look
- which means they can't be parallelized



