# Classification
Classify data


- classification requires a decision boundry
	-	form of supervised learning
	-	algorith is used on labeled data
	-	experts classify data
	-	focus is on finding a fitting _Decision Boundry_

## Methods
- Logistic Regression
- k-NN
- Decision Tree
- SVM with RBF-Kernel
- SVM with n polynomial kernel
- Gaussian Naive Bayes

## Simple Classification: k-Nearest Nieghbours
k-NN considerst k-nearest neighours for categorizaation.
Uses distance
k-NN works well with small data

### hyper parameter k
- performance calculatted with Confusion matrix or other
- _Hyperparameter tuning_ -> finding the best k
- data has to be normalized (otherwise unbalanced)

+ Easy to implement
+ ideal for small datasets
+ good baseline to comapre classifications 

- slow because everything has to be calulated always (no learning phase)


## Classifcation vs Regression
Classification --> predicts categories, done with decision boundry with categorical labeled data
Regression --> predicts values/numbers. Labeled data is numerical  and has relations.


 ## Logistic Regression
 binary, linear Classification-Problem
 
 ## Binary Classifcation
 1 Feature
 - Seperate data into categories
 - Binary classicaiton when only 2 values per label (Ex: _Yes, no_)
 - labeld with 1, 0
 - Decision Boundry is where line is set

2 Features
- higher dimension for classifcation
- when having 2 features is a line
- we need a linear binary classification
- For this logistic regression can be used
![[Pasted image 20220128140326.png]]


### Logistic Function
_Sigmoid_ can have a lot of different inputs / attributes but result will always be 1 or 0

- For logistic regression wi fit sigmoid. We receive probablity if it is 1 or 0
- ideal for probablity becasue value between 0 and 1
- Calcualted with the sum of features X$\theta$

## Cost function for Logisitc Regression
When Y = 1 we want g(X$\theta$) so close to 1 as possible.
- look for cost funcation that is low when g(X$\theta$)  close to 1 and high when close to 0
- Same can be done for Y = 0

![[Pasted image 20220128141634.png]]

Proof for Y=0 and Y=1

![[Pasted image 20220128141715.png]]


## Logistic Regression by Gradient Descent
Optimal parameter **CAN NOT BE FONUND WITH OLS**.
Need to use _Gradient Descent_


## Non-Linear Decision Boundries
With _Feature Engineering_ certain decision boundries can be found which are non linear.

![[Pasted image 20220128141945.png]]

## Multiple Classifcaiton
_One versus all _ concept
- classify always one group and then go to next
![[Pasted image 20220128142101.png]]


## Performance Analysis
Measure statistic of the errors to interpret the performance
- confusion amtrix

![[Pasted image 20220128142208.png]]

- make sure not to have imbalanced data. For example a lot of NO cases but little yes cases. Errors on yes cases might be high but can't be read from the _accuracy_

### Sensitivity - Performance on YES instances
Sensitivty measures how many times yes is predicted when the value is yes. 

_true posivite rate_ or _recall_

Sensitivity $=\frac{T P}{T P+F N}$


### Specificity - Performance on NO instances
Measures how many times no's are predicted in comparison to the true no's

Specificity $=\frac{T N}{T N+F P}$


### Precision - If True Negatives not Available
Sometimes there are no True Negatives. In that case we have to use _Precision_
How many times is the model correct when predicition YES

Precision $=\frac{T P}{T P+F P}$


### F1 Score
Is the average betwenn Preciion and Recal (Sensitivity) in percentage. By Design, F1 doesn't use True Negatives to calculate.

F1 is strongly impacted by wrong scores. Should be used when classification is used on steep data.

$F 1=\frac{2 * \text { precision } * \text { recall }}{\text { precision }+\text { recall }}$


#### Overview
![[Pasted image 20220128142827.png]]






## Performance Optimization
How can the performance be optimized?


- when data isn't separated enough then the model is UNDERFITTED ---> high bias
- when OVERFITTED, it has is too optimized on the training data. ---> high variance

### Bias and Variance

- __bias__
	- Systematic offset of the goal
	- model is consistent but not accurate
- __variance__
	- high spread
	- but accurate on average
	- inconsistent

![[Pasted image 20220128143202.png]]


### Tips for dealing with Bias and Variance

- __bias__
	- use bigger sets of features
	- diffrent set of features
	- feature engineering
	- use more complex algos
	- more data isn't always the solution
- __variance__
	- Reduce amount of features
	- more training data
	- data cleaning (remove outliers)
	- stop training earlier
	- Advanced:
		- Automatic reduction of features (Feature Selection)
		- Regularization
		- Ensemlbe Methods


