# Tree Model
supervised learning with _Decision- Regression Tree and Random Forests_

## Decision Tree
- classifiction problem
- example
	- Buyanalyse and based on that make assumptions/predicitons
- features
	- Shopping duration, Sale, date.
	- Algorithim chooses it own features 
	- Same values can have different impact


### which features?
Yes -> when a product was bought
No -> not bought
Algorithm has to filter which features lead to a yes


### Adding decisions
No instances can be divided further
![[Pasted image 20220129003455.png]]



## ID3 - Tree Consturciton Rules
Algorithm for constructing a decision tree

1. When there are only _Positive_ or _Negative_ instances --> stop splitting
2. When both have _Positive_ and _Negative_ instances are left --> continue splitting and adding new child
3. Special case: ---> no instances left ---> look at parent and decide
4. Special case --->When there are instances left but no Features ----> data is dirty. Bad Data Quality assessment


## Splitting Criterion
 Two approaches for splitting feature
 - choose whichever brings most information
 - gini index or Impurity, Instance with yes and no are unclean


## Gini Impurity
- measured for each feature
- Choose a random value for every label and calculate the probablity which move will be done
- Multiply probablity of Yes with probalbity of No
- Sum of both labels

![[Pasted image 20220129004336.png]]


### Gini Impurity for numerical values (Continues variables)

1. sort table
2. choose transition between 2 values and calculate the avg
3. Avg of Transitions used as Splitcriteria
4. Use avergages as feature values to calulate Gini Impurity

## Splitting Feature Sleciton and Stop Criterion
- the smaller gini impurity the better
- stop when Purity doesn't get better
- split done according to best features



## Regression Trees
CART Regression tree
- Regression valuers in the leafs
- measure variance of the data
- variance replaces gini impurity
- rest the same like decisin tree


## Advantage / Disadvantage of Decision and Regression Trees

+ easy to understand and easy to interpret
+ numerical and categoriacl values work together
+ works without almost no data preperation
	+ no need for normalization or dummy variables or no VSM
+ also work well on big datasets

- not very accurate
- tend to be overspecialized (danger of overfitting)
- Pruning must be considered (removing of knots)
- very sensitiv on dataquality
	- changes on datapoint can have big impact



## Random Forests
- collection of decision tree
- trained from the same training set
- random data / attributes are chosen
- result is various Tree with different subsets
- subtree help make prediction


### How to build a Random Forest

Training
1. choose random D* out of training data
2. choose random set A* out of Attributes A
3. create deciison or regression tree out of D* and A*
4. Repeat for as much as you wish

Decision Phase
1. Receive different prediciton for each tree
2. combine results for final prediciton (avg of regression or majorizy of classification)

Because of the randomness factor, the performance increasese --> decrease of overfitting




