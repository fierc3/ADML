# Neutal Networks

## The Perception - An aritfical neuron
- neuron
	- a cell that can be activated via electricity and can communicate via Synapses
	- A neuron has
		- Soma, Dendriten (Receptors) and a single Axon
		- Input signal --> dendriten
		- Output --> Axon
		- Learning
			- Synapsis firing together (wire together)


## Perceptron as a Logic Unit
- _Inputs X_ from previous Axons are multiplied with weights from synapse
- Bias _B_ allows extra tuning on activation layer
- Active when true --> reached certain threshold
- Bias and Weights are learned with labed data and help of gradient descent
- _function g(z)_ is called _Activation Function_
	- Sigmoid, Softmax

### Linear vs Non-Linear
single linear --> can't implement XOR

![[Pasted image 20220129123627.png]]

## Feed Forward Neural Networks
left 2 right --> _feed forwad network_

### Adding a second Neuron
![[Pasted image 20220129123659.png]]

### Adding third Neuron
![[Pasted image 20220129123740.png]]


### Single Layer Neural Network
-> solves linear seprable Classification problems
If _g(z)_ has a logistic function then every output _a_ has a _One vs All_ classification --> Klasse 1 or not, etc

### Introducing a Hidden Layer
When NN has _Input, Hidden and Output layer_ we call it a _two layer  network_

### Hidden Layers Create new Features

![[Pasted image 20220129124137.png]]


### Universal Approximators
- Every type of Non-Linear classification can be done in a Multilayered Network
- Hidden layers learn new features, which probably haven't been found before
- training slow--> but implementation is fast
- Multi Layer Artifical Neural Networks are universal approximations (_approxiamte any conitnous function_)
- _Regularization_ helps against overfitting
- error resistant and robust


## Deep Learning
- hidden layers are the core aidea in deep learning
- new features can be taught itself
- no need for expert knowledge



## NN Training by Gradient Descent and Back-Propagation
the *magic*

### Feed Forwad Error Calculation
in supervised liearning
-> we use labeled data and see what the result ist
If bad result
-> Adjust Parameter _W_ and _b_
We choose the weights W and biases b based on minimizing the cost function

$J\left(W^{[k]}, b^{[k]}\right)=f(y-\hat{y})$


### Choosing a cost function
Choose cost function depending on the underlying problem

- Regression
	- MSE
	- MAE
	- MASE
- Binary Classification
	- Cross Entropy
	- Hinge Loss
	- Squared Hinge Loss
- Multi Classification
	- Multilabel Cross Entropy
	- Kullback Leibler Divergence


### Recap Feed Forward Neural Network
![[Pasted image 20220129125945.png]]



## Activation Functions and Soft-Max Classifier
for useful outputs wenn need a non linear acivation functions

### Choosing an Actiation Function
- goal output would be a probablity between 0 and 1
- since were using _gradient descent_
	- the activation function should be _steady_ and _diffenrentialble_
	- when funcation is flat --> weakers gradients --> slow
- when function is _monotonic_ (in only one direction) ---> then single layer model _convex_
- Good example ---> __Sigmoid__

### The Recitifed Linear Unit (ReLU)
partly linear and often used with _Hidden Layers_.
- values larger than 0 = value
- values smaller than 0 = 0
- Default activation function for a lot of NN
- ReLU is easy to train and have good results
- But leat to _dying units_ problem --> because negative values are ignored
- ![[Pasted image 20220129131400.png]]

### The Leaky ReLU
most used function in DL Models, because gradients don't dsiapper
- leads to non-zero activation for negative values
	- solves _dying units_ problem
- Good perormance but mixed results
- Generally good for Hidden layer Activations
- ![[Pasted image 20220129131407.png]]

### Soft-Max for Multi-Label Classification
Used as Output for multi-labels
- softmax modifies a vector of _n_ reellen values --> leading the sum to be 100%
- softmax is a generalization of the logisitc function for multiple dimensions
	- also used in _mulinomial_ Logistic Regressen
	- Usually used in the last Activation Functions of a NN
	- input: _logits_
	- function normalizes the outputs of a network into a probablityspread

![[Pasted image 20220129132711.png]]



## Summary Activation Functions
![[Pasted image 20220129132727.png]]




