# Regression

Goal, is to predict attribute by analyzing labeled data.

Variants:
- Linear Regression
- Polynomial Regression
- k-NN Regression
- Support Vector Regression
- Regression Tree and Random Forests
- NN
- XGBoost Regression


## Measuring Regression Quality
Which model has best quality
#### Linear
Residues minimize Errorterm

#### any
minimize the combined residues

### How to measure Regression Errors
Residues is the difference of guessed and effective value.
Sum of these residues would be useless because of positive and negative values.
Better ---->
- MAE - Mean absolute error
	- Absolute values
- MAPE - Mean Absolute Percentage Error
	- Better than MAE, percentage based
- MSE
	- Mean Squared Eror
		- Benefit -> big differences will be even bigger and more punished
		- harder to interpret than MAPE/MAE


### Comparison with Mean approximation
Better or worse than mean prediction?

#### R-Squared R^2 
R^2 explains how much of the variance can be explained by the model.
Between [0,1]
0.53 = 53% of the variance can be explained by the model
Goal -> maximize r^2 while minimizing regression error (MAE, MAPE, MSE)


## ML Quality Assessment
Mostly look at the R^2 value.

### Generalization error
Only because it peforms well with training data, doesnt mean itll perfom well in general

-> overfitted when too optimized on our training data

### Perfect Performance on Training Data
Can predict training data perfectly but a sign of overfitting.
Were more interested in the Tendencies then actual values 


## Simple workflow
![[Pasted image 20220127145530.png]]


## Hyperparameters
"Configuration" of a ML Model.
example: k-NN - the k parameter
Job is to find hyperparameter which is GOOD for __UNSEEN__ data

_Parameters_ are automatically optimized


# HOW TO GET FIRED AS A DATA SCIENTIST
Comparisons of hyperparameters HAS to be only done when testing with  truly unseen data


## Workflow for Hyperparameter Optimization
Split 60 / 20 / 20 (training / validation / test)
Only use test for FINAL performance test

1. loop through hyperparameter combinations
2. train with training data
3. measure performance with validation set
4. choose model with best performance
5. final test with validation set


## K-Fold Cross-Validation
Used when not enough data for 60 / 20 / 20
use 80 / 20
80% used for kFold cross validaiton 

1. 80% seperate in k-parts
2. training done on k - 1 parts
3. After each round a different Validaiton and training set used.
4. Accuracy is the average of all rounds


 ![[Pasted image 20220127153921.png]]