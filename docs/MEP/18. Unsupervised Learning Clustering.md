# Unsupervised Learning: Clustering
Data without labels and no relationships to the goal variable


## K-Means Clustering
- for k-means data have to be in a _Vector Space Model_
- often used to find targetgroups
- scales well


**!DATA POINTS HAVE TO BE NORMALIZED!**

### Cluster
Categorization of Values in VSM

### Preparation
- choose number of cluster _k_ (hyperparameter)
- for every  _k_ theres a random number
	- Add to data cloud
	- the points are cluster centers


1. Serach for each point the nearest point (_euclidean distance_) ---> points added to center
2. Center adjusted for new MEAN of all points
3. Iterate through 1 and 2 until its stablilized (centers doent move anymore), until they converge

![[Pasted image 20220202135830.png]]



### Clustering Distortion
- measures quality
- _Total Distortion_
	- distances sqauared between all points and center summed up
	- $\sum_{i=1}^{n}\left\|x_{i}-\mu_{c_{i}}\right\|^{2}$
- _Average Distortion_
- Total distiortion / _n_     , n = number of clusters

#### Convergence and Optiamlity
- k Means approximates the optima lsolution
- converges ALWAYS
	- must not be global, can also be a local minimum

#### Choose the number of clusters
- _k_ -> number of clusters dependant on data
- elbow method leads to ideal _k_
- ![[Pasted image 20220202145550.png]]



## Agglomerative Clustering
Hierarchial clustering

- initial --> every point is a cluster
- calculate for each cluster pair the distance and merge the next 2 clusters
- loop until stop

Configuration options
1. Distance measure: type of distance / similarity
2. Linkage: Merge of endpoints
3. Stop criterias:
	1. thershold on cluster count, cluster density


### Endpoint for Distance MEasurement
- different varianats
	- simple link = min dist
	- complete lin = max dist
	- average link = cluster center
	- ward link = min varaiance inside of acluster + max variance between clusters
- different then distance measure
- don't merge when high density (example 4)

![[Pasted image 20220202150627.png]]


### Dendrogram
Shows merging steps for every step.
Distance values can be used a stop criteria, evaluated by the elbow method


### k-Means != Agglomerative CLusterings
- In k-means the cluster datapoint can jump
- In agglomerative its always the same
- k-Means has 1 stopcriteria, the amount _l_
- k-Means guarantees Convergence with only Euclidean Distance
	- AC with any distance measure
- k-Means scales with large datasets, ac only with small
- k-Means produces different cluster depending on the initialization
	- AC always same
- k-Means harder to interpret ---> AC understandable thanks to Dendogram

![[Pasted image 20220202151001.png]]






