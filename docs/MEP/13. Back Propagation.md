# Back Propagation
used to train NN and increase speed of Calculation of Gradient Descent

A lot of models
- NN
- CNN
- RNN
use BP for training. Basically everywhere there are multiple layers.
BP is difficult, most of time time build in open source ML Framework

## The Chain Rule
Method to find the differntial of a combined function. Output of a function is the input for the next


### The Logistic Function
_Sigmoid_
Differential of Sigmoid is easy to calculate

$g(z)=\frac{1}{1+e^{-z}}$
--->
$\frac{d g(z)}{d z}=g(z)(1-g(z))$



### Mutlivariate Chain Rule
For Multivariate Functions a partial differntial chain rule can be applied (multi variables)

$\frac{d f}{d t}=\frac{\partial f}{\partial x} \frac{d x}{d t}+\frac{\partial f}{\partial y} \frac{d y}{d t}$


## Gradient Descent again
Gradient descent training
![[Pasted image 20220129140710.png]]


## Back Propagation
Calculate gradient at every layer

![[Pasted image 20220129140748.png]]
![[Pasted image 20220129140802.png]]


### Familiar Cost & Activation Functions
To use BP wee need a cost function _J_ and activation function _g_
--> out of them we receive the gradient to optimize cost function
- the requiremnts for this to work is that both function have to be differntial exp: _MSE_ or _Logistic Function_

### Multi Variate Chain Rule
because the Cost function _J_ is based on _z_ of every level, a partial Chain rule has to be used.
![[Pasted image 20220129141053.png]]





