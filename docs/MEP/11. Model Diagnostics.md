# Model Diagnostics
optimize Model

## Model Training
- supervised learning --> most training with Gradient Descent
- training checks the cost function and how it develeops


### Training Curves
- show development of cost function
- goal -> converge as fast as possible 
- check the Dynamic of the curve and its perfromance during training

### Training Epochs
- 1 epoch = 1 cyckle of processing all data
	- Batch: 1 epoch = 1 itertaion
	- Stochastic: 1 epoch = N iteration


### Mini Batch Gradient Descent
- _Mini Batches_ = Subsets
	- we choose an amount (usually 32) and divite it by B
	- this leads to the count of iterations of B
	- _Mini Batches_  give an Performance boost in Batch- and Stochastic Training.
	- It converges faster


#### Epoches vs Batches
- _Epoch_ : whole dataset
- _Batches_ : subset where the gradient is calculated and new weights


### Step-Size Tuning
- _a_ can be tuned to influence Gradient Descent
- _a_ the smaller -> the longer but more accurate
- _a_ if too big it could miss optimal



## Model Validation: Learning Curve Diagnostics
after training --> validation (awlays unseen data)
- WHat to do when bad performance during validation?
	- repeat training, but not mindlessly
		- models mostly _underfitted_ (high bias) or overfittet (high variance)

### Learning Curves
Used to diagnose under- & overfitting
#### Diagnosing Under-Fitting (High Bias)
- Model too simple / bad performance
- more data wont help
- more complex model needed
- more features
![[Pasted image 20220129101339.png]]

#### Diagnosing Over-Fitting (high variance)
- Happens often
- hard to solve
- more data and clean data
- maybe smaller set of features
- or try _Regularization_
![[Pasted image 20220129101353.png]]



## Model Tunning: Regularization and Other Tricks
the higher the dimension of training features the higher the probablity that we overfit
_Curse of Dimensionality_

#### Regularization
- helps punish big parameter which are unimportant
#### Cross Validation for Regularization Tuning
Because $\lambda$ is hyperparameter, we can use cross validation
futher tricks:
- Tuning Stepsize or Batch size
- tuning hyperparameter
- use validation data for diagnosis
![[Pasted image 20220129101903.png]]


## Ethical Considerations

#### Questions
- Where will the application be used?
- Are the data biased?
- who gives the data? Are they trustworthy?
- Do the people know their data is being used
- do we have the rights?


#### Bias in the data
- big problem
	- reality not always mirrored in data
		- Skin color, traditions etc
![[Pasted image 20220129102123.png]]

#### Ethics Guide for Business
- use control instances
- "Dual-Use" --> try abuse a good idea
- Audits
- Guarantee explainablity
- Receive feedback from person with ethical background


### ML Workflow

1. _Data Quality Assessment DQA_ is always woth it
2. Ethics is not an after thought --> address from the beginning
3. Split data into --> train, validate and test sets --> lock away your test set
4. Normalize Data, Visualize Data ---> Understand your data
5. Define performance metrics you want to use for you appliaction
6. Implement a quick- and dirty baseline classifier or regressor
7. Examine your training curves and finetune your training
8. use your validation set for model diagnostics with learning curves
9. learn and improve e.g hyperparameter tuning, cross validation
10. ONLY USE YOUR TEST DATA ONCE AT THE END










