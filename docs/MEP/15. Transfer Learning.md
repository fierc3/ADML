# Transfer Learning
- Filter in Deep CNN lead to abstract features --> edge filter, color blobs etc.
- the further right in the NN the more complete is the feature.
- final layer has _Prototype Objects_
- NN ---> _Feature Extractors_, called backbone = all Hidden without output

- to train a good model, a deep network is needed.
- to use train we need labeled data
- first phase as _pre-training_ (recognize human face)
	- then domain specific (recognize my face)

## Approaches
- Datasets on Wikipedia
- Data set search on Google
- Kaggle

### Libraries of pre-trained models
- tensorflow has pretrained models
- implemented for transferlearning

### Model Repurposing
1. Use pre trained model to detect objects
2. Seperate last laser for object localitation
3. Use new object localisation layer with 2 categories added
4. Just train last layer with domain specific Dataset

### Pros and Cons
+ very efficient, because only training one layer
+ least data hungry
+ reuse of backbone
	+ _frozen layers_

- backpropagation not possible because of frozen layers
- backbone has to fit
- if unhappy with result ----> _Fine-Tuning_


## Fine-Tuning
- don't just train last layer but a few last layers
- unfreezing few of the last layers of the backbone
- Weights arent random ---> defined of pretraining
- Optimization doesnt start from zero (_Transfer Learning Effect_)
- Training becomes data / resource hungrier

### Transfer Learning
--> solves problems of cost and size of datasets


## Issues with Supervised Learning
- not data set size but
	- lack of annotations
		- labelled image datasets
		- labelled video datasets
		- labelled text datasets
		- labelling quality --> Different opinions even for experts


## Unsupervised Pre-Training
Current reasearch topic
- Pretraining needs a lot of data
- Data is there but labeling does not scale


### Masked Language Modeling
- use giant set of unlabeled sentences in natural langualge and masks random words
- pre trainned model is used to train those sentences and masks
- trick is --> we know what was masked and can thus optimise


### Contrastive Learning
- for pictures or videos
- big set of unlabeled data ---> from social meda for example
- not calculating a value but compaing pictures
- Sample 2 _Tiles_ from a picture ( A and B). 
	- All B's will be in one batch
	- We expect high similaritty between A and B when its the same picture and low similarity when its another
- Same with videos just done with 2 samples with same duration





