# ML Fundementals

3 ML Disciplines

## Supervised ML
Data annotated before hand and defined by domain expert if wrong or correct.
- medical image analysis
- product recommendations
- prediction of selling prices for the RE Market

## Unsupervised ML
Network has to learn its own features
- finding target groups with help of clustering
- Market basket analysis using assosciation rules


## Reinforcment ML
Feedbackloop um nÃ¤her zum ziel zu kommen
- learning to play chess by itself
- learning to play jass by itself

### All based on VSM
#### Text
How to encode text numerically -> TF-IDF Scores -> turnes words  to numerical vectors

#### Distance & Similarity
the closer the more similar, the further the more distant
Everything is based on Similarity, except NN & Decision Trees
Calculated in VSM

##### Euclidiean Distance or L^2 Norm
General form of Pythagoras
x-Dimensions possible. Any number between [0 & inf]
Find closest point through minimizing

##### Semantic of Similarity
Depending on data different distances/similarities have to calculated differently.

##### Cosine Similiarity
Calculates Angle between two vector. 

##### Euclid vs Cosine
Einheitskreis / unit circle normalized equals same value.

##### Manhattan Distance
When a to b isn't directly possible. (Streets of manhattan)

##### Jaccard Similarity
ð‘—ð‘Žð‘ð‘ð‘Žð‘Ÿð‘‘(ð‘‹, ð‘Œ ) = |ð‘‹âˆ©ð‘Œ| : |ð‘‹âˆªð‘Œ|

##### Haversine Distance
For Geo data, considers the bend of the earth

#### Points to Distributions
Someitmes DIstrubutions have to be compared to a point (and vice versa)
- Disturbutions is normalized
- scaled -> variance = 1
- End by calculating eucleadian distance betwenn point x and avg

## Normalization
### Min-Max Normilization
Transforms data into interval [0,1]. Smallest value = 0, largest value = 1
- always positive numner
- can be seen as percentage
- not possible for supervised learning -> since we dont know global min/max


### z-Score Normalization
Data transformed so that avg is 0 and standard deviation = 0
- works for supervised and unsupervised learning
- not percentage interpetation (which min max has)
- can have negative values
- to interpretate, data has to be transformed back

### Normalization Parameters
- MinMax
		- min, max
- Z Score
	- mean, std
- REMEMBER
	- Always define parameters based on trainingsdata


## k-Nearest Neighbors Classification (k-NN)

- one of the easiert ML-Algorithms to grasp
- Method for Regression and classification
- perfect example to show how important distance/similarity is
- parameter _k_ = means that it looks for 3 nearest points to a new point
- k = 1, for example would jus take the nearest

### KNN Regeression
LÃ¶st regerssions problem, for example price prediction

### Hyperparameter
__a parameter to control the learning__
- In Knn _K_ is a hyperparameter -> count of neighbors
- Result can differ drasticly depending on the chosesn K


### KNN Facts
- Very slow, so many similarity calculations ahve to be done (for every data point)
- Goodbaseline for small dataset
- all neighbors have same weight, even doe the differences could be different
- Alternative the distance can be considered whe n using the parameter ð‘‘ = distance > 0
- needs the least amount of data
- k is the hyperparameter


## Data preparations for Recommender Systems

### How Strings can be processed

1. __convert to lowercase
2. __Tokenizing
	1. Cleans the texts
		1. Recognizes shortcuts
		2. Recognizes special characters
		3. expands shortend versions l'auto -> la auto
		4. Can guess complexe formats, like telefon mnnumbers, data etc
		5. Can normalize data
		6. Can classify tokens
3. __Lemmatization
	1. Words to grundform / infinitve
4. __Stemming
	1. Process to summarize variantes of a word to its wordbase / wordstamm
		1. gsehen oder sah to sehen
		2. Famous framework is Snowball by Martin Porter
5. __Created Test and Training data out of the normalized data
