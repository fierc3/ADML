 # Linear Regression
 
 relationship between 2 variable which is steady (atleast in a period)
 
 _Goal_: to find relations between features, if they exist. If it a reltion can be approximated _linear_ then we can call it it linear regression
 
$\hat{Y}=\theta_{0}+\theta_{1} X$


__Common for Predictions__
How to find $\theta_{0}$ & $\theta_{1}$ to predict next value?
- SUPERVISED LEARNING
- $\theta_{0}$ = _Bias term_, moves it in the y axis. Nothing to do with under / overfitting
- minimize _fitting error_ by choosing $\theta_{0}$ & $\theta_{1}$
- Regression errors can be found with
	- MAE
	- MAPE
	- MSE 

## Defining a cost function
MSE is fav to calculate Regression errors.
- is convex (bowl shaped)
- steady differential for both paramerts (theta 0 and theta 1)
- We call it = $J(\theta_{0}$ , $\theta_{1}$)

$J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{N} \sum_{j=1}^{N} \epsilon_{j}^{2}=\frac{1}{N} \sum_{j=1}^{N}\left(y_{j}-\hat{y}_{j}\right)^{2}=\frac{1}{N} \sum_{j=1}^{N}\left(y_{j}-\theta_{0}, \theta_{1} x_{j}\right)^{2}$

![[Pasted image 20220128123717.png]]

### Minimizing cost function
Cost function is convex -> bowl shaped.
Global minimum can be found with _Calculus_

### Ordinary Least Squares
Finding the minimum by looking for Gradient 0

![[Pasted image 20220128124121.png]]

### Linear Regression by Gradient Descent
- sometimes more efficient for bigger datasets
- View _Gradient Descent_



 ## Regression performance $\left(R^{2}\right)$
 
 We have to check if it actually is linear
 
 - Variablitity of _Y_ of the line is is _MSE_
 - Variablitity of _Y_ of the MEAN is the variance
 - Relation between those 2 is good indicator for performance

### $\left(R^{2}\right)$
is a measuremnt to see how strong the variablitiy of Y is explained by X.
1- $\left(R^{2}\right)$ explains how much of the variablitiy unexplained is.

$\left(R^{2}\right)$ can't be trusted 100%. Always __PLOT/VISUALIZE__.

### Visualize Residuals
should look like white noise
$\mathbf{y}=\mathbf{X} \Theta+\epsilon \Longrightarrow \epsilon=\mathbf{y}-\mathbf{X} \Theta$****

### Correletion and R^2
correlation is a general measure how  2 variables react to each other.
_Pearson correlation_ is a special case, it only considers linear Relations between x and y.
- normalized value of covariance, lies between [-1, 1]



## Multiple Linear Regression
N-Dimensional cases

### Matrix with M-Regressors
![[Pasted image 20220128125212.png]]


### Regularization
**IMPORTNAT TO REMEMBER**
Important not to overfit by using too many extra features.
This leads to generalization errors when new data is used.
Focus on features that have to most influence.
				_thats regularization_
				
### Non Linear Regression

![[Pasted image 20220128125508.png]]


### Ridge and LASSO
Both methods for regularization
Both add to _OLS_, the cost function, an extra term, to less the weights of unimportant features.

- Ridge: uses Sum of of square of the parameters, irrelevant features will be way smaller
- LASSO: uses the sume of absolutes of the paremeters, irrelevant features will be 0

### Regularization Parameter $\lambda$
Big complex parameter will be penalized. Brings the effect, that only important features will impact the regression.
THis can be controlled over the $\lambda$ parameter. Is $\lambda$ chosen too big, then the model can be underfitted (so too general).


