# Generative Models

## Language Models
- caclulates probablilitydistribution
- every word has a probabliity
- $p\left(w_{3} \mid w_{1}, w_{2}\right)=\frac{p\left(w_{1}, w_{2}, w_{3}\right)}{p\left(w_{1}, w_{2}\right)}$
- to predict w3


### 2nd Markov Chain
- only remember the last 2 words
	- leads to senseless sentences


### Text Generation with RNNs
- can remember connections between words without exponential growth
	- statusroom is continous
	- the words are non linear combination of weights
- Good for short sentences
	- long text --> vanishing gradient problem


## Transformer Networks
State of the art

### Attention motivated from Information Retrieval
- information search based on key-value DB
- Search word _q_ is mapped to Key _k_ and gives value _v_
	- $\operatorname{attention}(q, \mathbf{k}, \mathbf{v})=\sum_{i} \operatorname{sim}\left(q, k_{i}\right) * v_{i}$

- Query, Key and values are NN vectors
- Similarity and Attention weights are scalar
- Ouput is a vector
- Softmax allows binding of 0 and 1
- Dot prodcut is a projection where we project Query on keys

![[Pasted image 20220202124927.png]]


### 17.2.2 Multi-Head Attention
- 8 layers are used
- ![[Pasted image 20220202125046.png]]
- comparision single and mutli head
- ![[Pasted image 20220202125105.png]]


### The Transformer
![[Pasted image 20220202125246.png]]





## GAN: Generatie Adversarial Networks
- 2 different NN
- 1 generates data which is either false or real
- the othe checks if data is real or false
- train each other
- 1. net
	- wants to improve creation of false data
- 2. net 
	- wants to detect it better


### Definition
- data out of unknown distrubtior
- wants to maximize distrubtion
- cost function defined as MinMax
- Training until _Nash Equilibirum_

#### Nash Equilibirum
- network cant optimize its cost function without changing the parameter of the other networks
- None of the networks can improve unilateral


#### Math behind GAN
- hard to train
- parameter swing / ostilize, are instable and never converge
- not konvex
	- hard to optimize cost function
- Discriminations need at the beginning --> because Gradients disapper form the geneerator 
	- both dont learn


#### Use Case GAN
- when reached _Nash Equilibirum_
	- remove disciriminator
	- predict only false
		- example: deep fakes







