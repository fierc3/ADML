# Gradient Descent
Trying to get to a local minima


## finding local minimum

Wenn reducing cost function -> were looking for  local or global minima
To change a parameter, another has to be changed -> Parameter Tuning.

Parameter θ (Theta) can be tuned.
J(θ) is the cost function, thus cost function can be indirectly controlled by θ

### Multi Dimensonal Vectors
In multi dimensonal functions the costfunction has a minima which is a vector. To find it we need the partial differential.

### Contour lines
Contour lines show the value of θ when J(θ) = c


## Batch Gradient Descent
Gradient is a vector which contains the partiel differntial of cost function J(θ) 

Batch = Used on a big data set

1. Calculate gradient of starting point
2. move as small step a in the exact opposite direction -a - J(θ)
3. go back to first step -> continue until local minima

Important -> normalize data first

- Costfunction = J(θ) => sum of  all Residues.
- ▽J = Batch Gradient Descent
- Iterative process based on the gradient
![[Pasted image 20220127160909.png]]

## Stochastic Gradient Descent
- All data points used for EVERY iteration
- makes sure we don't miss anything but can be slow with big data sets
- Alternativ ----> _stochastic gradiend descent_
	- uses only 1 datapointy (1 sample of a dataset) to estiamte the gradients
	- point randomly chosen

![[Pasted image 20220127160859.png]]


## Different approaches visualized
![[Pasted image 20220127160952.png]]


 ## batch vs stoch
 
 ![[Pasted image 20220127161031.png]]
 
 
 ## defining Learning Rate a
 
 alpha a is the step size
 - normally between 0.01 < a a 0.1
 - if too big --> possiblity of jumping over minimum
 - if too small --> too slow
 - ideal: start with big a and become smaller


## stopping criteria
- stop when maximum of iterations has been reached
- cost function reached threshold
- reduction of costfunction between iteration is small
- gradient of cost function is small
- steps between iteration is smaller than certain thershold



## Applicaitons of Gradient Descent in ML
finding optimal parameter for linear or lgoical regression, for finding weights of NN or improving of billion of parameters in deep learning


### Linear regression by gradiend descent
tries to fit a line to datapoints
-> 2 parameter = xyz panel --> 3+ dimensions = hyperplane

### Logistic Regression by gradient descent
sigmoid-data for binary classification

### NET training by Gradient Descent
Weights are optimized to lower classification errors. Cost funcation is non-convex. With back propagation the speed is increased.

### Deep Learning by Gradient Descent
A lot of data
cost function is created which optimizes the weight. BAck progpagation can help.