{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "In this exercise, several parts of the code are missing, which should be completed by you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Skin disease dataset using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us apply a neural network on the skin disesase data. To reduce the training time we reduce the amount of data in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>t0</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18595</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>148.854843</td>\n",
       "      <td>136.881622</td>\n",
       "      <td>136.502075</td>\n",
       "      <td>0.970898</td>\n",
       "      <td>0.958832</td>\n",
       "      <td>3.940685</td>\n",
       "      <td>0.845263</td>\n",
       "      <td>-0.627514</td>\n",
       "      <td>1.935074</td>\n",
       "      <td>1.260257</td>\n",
       "      <td>1.217211</td>\n",
       "      <td>4.960808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215838</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143.600128</td>\n",
       "      <td>141.486252</td>\n",
       "      <td>104.843246</td>\n",
       "      <td>2.135985</td>\n",
       "      <td>3.054679</td>\n",
       "      <td>21.422846</td>\n",
       "      <td>-2.388975</td>\n",
       "      <td>-2.513810</td>\n",
       "      <td>-21.041121</td>\n",
       "      <td>3.078798</td>\n",
       "      <td>4.343002</td>\n",
       "      <td>27.046103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434611</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.094589</td>\n",
       "      <td>145.108856</td>\n",
       "      <td>153.140991</td>\n",
       "      <td>1.710095</td>\n",
       "      <td>1.765065</td>\n",
       "      <td>7.265410</td>\n",
       "      <td>1.693099</td>\n",
       "      <td>0.905624</td>\n",
       "      <td>-7.121035</td>\n",
       "      <td>2.588367</td>\n",
       "      <td>2.367739</td>\n",
       "      <td>11.046728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469053</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>147.475906</td>\n",
       "      <td>143.886978</td>\n",
       "      <td>140.092804</td>\n",
       "      <td>1.171563</td>\n",
       "      <td>1.795381</td>\n",
       "      <td>12.887458</td>\n",
       "      <td>0.685154</td>\n",
       "      <td>1.610793</td>\n",
       "      <td>-12.241491</td>\n",
       "      <td>1.512837</td>\n",
       "      <td>2.441238</td>\n",
       "      <td>17.298553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214172</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>148.494354</td>\n",
       "      <td>143.794174</td>\n",
       "      <td>123.755501</td>\n",
       "      <td>0.833631</td>\n",
       "      <td>1.102370</td>\n",
       "      <td>9.927114</td>\n",
       "      <td>-0.851710</td>\n",
       "      <td>-0.780081</td>\n",
       "      <td>-5.027682</td>\n",
       "      <td>1.270171</td>\n",
       "      <td>1.543895</td>\n",
       "      <td>11.943620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        class   t0   t1          t2          t3          t4        t5  \\\n",
       "18595       0  0.0  1.0  148.854843  136.881622  136.502075  0.970898   \n",
       "215838      0  0.0  1.0  143.600128  141.486252  104.843246  2.135985   \n",
       "434611      0  0.0  1.0  146.094589  145.108856  153.140991  1.710095   \n",
       "469053      0  0.0  1.0  147.475906  143.886978  140.092804  1.171563   \n",
       "214172      0  0.0  1.0  148.494354  143.794174  123.755501  0.833631   \n",
       "\n",
       "              t6         t7        t8        t9        t10       t11  \\\n",
       "18595   0.958832   3.940685  0.845263 -0.627514   1.935074  1.260257   \n",
       "215838  3.054679  21.422846 -2.388975 -2.513810 -21.041121  3.078798   \n",
       "434611  1.765065   7.265410  1.693099  0.905624  -7.121035  2.588367   \n",
       "469053  1.795381  12.887458  0.685154  1.610793 -12.241491  1.512837   \n",
       "214172  1.102370   9.927114 -0.851710 -0.780081  -5.027682  1.270171   \n",
       "\n",
       "             t12        t13  \n",
       "18595   1.217211   4.960808  \n",
       "215838  4.343002  27.046103  \n",
       "434611  2.367739  11.046728  \n",
       "469053  2.441238  17.298553  \n",
       "214172  1.543895  11.943620  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"skin_disease.csv\")\n",
    "df = df.sample(frac=1)\n",
    "df = df.iloc[0:100000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"class\"])\n",
    "y = df[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Split the data into a train and test set. Use 40% of the data for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our Multi Layer Perceptron with 2 hidden layers. This time we use the [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) implementation from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,15),\n",
    "                    activation='relu',  # activation function\n",
    "                    solver='adam',  # optimizer\n",
    "                    batch_size=1024)  # size of minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train the neural network on `X_train`, `y_train` and plot the loss by accessing the attribute `loss_curve_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_costs(costs):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(costs)\n",
    "    ax.set_title(\"Loss curve\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)\n",
    "plot_costs(mlp.loss_curve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement your own predict function. For that we'll need an activation function for the hidden layers, in our case `relu` and for the output layer `sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x) #works also with just checking if value negative then equals 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def predict(mlp, X):\n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "    # define the first activations, e.g. inputs\n",
    "    \n",
    "    # forward propagate through layers\n",
    "            \n",
    "    # last layer output activation `sigmoid`\n",
    "    \n",
    "    # transform to 1-D and threshold\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def predict(mlp, X):\n",
    "    # define the first activations, e.g. inputs\n",
    "    A = X\n",
    "    \n",
    "    # forward propagate through layers\n",
    "    for i, (W, B) in enumerate(zip(mlp.coefs_, mlp.intercepts_)):\n",
    "        z = A.dot(W) + B\n",
    "        # if hidden layer, apply `relu`\n",
    "        if i != mlp.n_layers_ - 2:\n",
    "            A = relu(z)\n",
    "            \n",
    "    # last layer output activation `sigmoid`\n",
    "    out = sigmoid(z)  \n",
    "    # transform to 1-D and threshold\n",
    "    out = np.squeeze(out)\n",
    "    out = np.array(out > 0.5, dtype=int)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Test your implementation with the scikit-learn predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# y_pred_scikit = ...\n",
    "# y_pred_own = ...\n",
    "\n",
    "# print('Are the outputs the same: %s' % ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the outputs the same: True\n"
     ]
    }
   ],
   "source": [
    "y_pred_scikit = mlp.predict(X_test)\n",
    "y_pred_own = predict(mlp, X_test.values)\n",
    "\n",
    "print('Are the outputs the same: %s' % (y_pred_scikit == y_pred_own).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Predict the values on the test set and calculate the accuracy and the f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 1024, 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (30, 15), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Accuracy: 0.9468\n",
      "F1: 0.6743\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(mlp, X_test.values)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(mlp.get_params());\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"F1: %.4f\" % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Skin disease dataset using `TensorFlow`\n",
    "In practice, the MLP from `scikit-learn` is never used because of the lack of customisation and the absence of GPU training. `TensorFlow` is a library specialised in deep learning and therefore also has implementations for advanced techniques. Thus the section below is a quick introduction to how the same network can be implemented using `TensorFlow`. The networks' results do not need to be the same, since as mentioned above, the `scikit-learn` implementation can not be as customised as the `TensorFlow` one. \n",
    "\n",
    "If `TensorFlow` is not already installed on the environment, it can be done using the \"magic\" cell from below. If it is already installed, make sure to use version `2.3.1`. Higher versions should also work but weren't tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.3.1 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (2.3.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in e:\\software\\anaconda\\lib\\site-packages (from tensorflow==2.3.1) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in e:\\software\\anaconda\\lib\\site-packages (from tensorflow==2.3.1) (1.11.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.3.1) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.3.1) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in e:\\software\\anaconda\\lib\\site-packages (from tensorflow==2.3.1) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in e:\\software\\anaconda\\lib\\site-packages (from tensorflow==2.3.1) (0.35.1)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.3.1) (2.7.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.3.1) (1.41.1)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.3.1) (0.3.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.3.1) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in e:\\software\\anaconda\\lib\\site-packages (from tensorflow==2.3.1) (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.3.1) (3.19.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in e:\\software\\anaconda\\lib\\site-packages (from tensorflow==2.3.1) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.3.1) (1.1.2)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in e:\\software\\anaconda\\lib\\site-packages (from tensorflow==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in e:\\software\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (50.3.1.post20201107)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in e:\\software\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in e:\\software\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mike\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in e:\\software\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.3.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in e:\\software\\anaconda\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in e:\\software\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in e:\\software\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\software\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in e:\\software\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.7.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in e:\\software\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in e:\\software\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in e:\\software\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\software\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in e:\\software\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in e:\\software\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__ == '2.3.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model in `TensorFlow` can be implemented using the `Sequential API`, which enables for easy extensibility by calling `.add()`. To implement the same MLP as above, we can sequentially add `Dense` layers to the model. Here the customization possibilities compared to `scikit-learn` is evident. For example, the activation function can be set for each layer separately, which was impossible before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                450       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 931\n",
      "Trainable params: 931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dataset_dim = X_train.shape[1]\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(30, input_shape=(dataset_dim, ), activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(15, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model, it needs to be compiled using an optimizer and loss function. In our case, we'll use adam as optimizer and binary cross-entropy as loss. Now the model can be trained by specifying the number of epochs and the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 2.0431 - accuracy: 0.7666\n",
      "Epoch 2/150\n",
      "59/59 [==============================] - 0s 915us/step - loss: 0.3863 - accuracy: 0.8850\n",
      "Epoch 3/150\n",
      "59/59 [==============================] - 0s 864us/step - loss: 0.3196 - accuracy: 0.9056\n",
      "Epoch 4/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.2912 - accuracy: 0.9056\n",
      "Epoch 5/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.2680 - accuracy: 0.9056\n",
      "Epoch 6/150\n",
      "59/59 [==============================] - 0s 932us/step - loss: 0.2478 - accuracy: 0.9056\n",
      "Epoch 7/150\n",
      "59/59 [==============================] - 0s 898us/step - loss: 0.2315 - accuracy: 0.9067\n",
      "Epoch 8/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.2208 - accuracy: 0.9108\n",
      "Epoch 9/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.2130 - accuracy: 0.9139\n",
      "Epoch 10/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.2073 - accuracy: 0.9154\n",
      "Epoch 11/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.2042 - accuracy: 0.9165\n",
      "Epoch 12/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.2000 - accuracy: 0.9173\n",
      "Epoch 13/150\n",
      "59/59 [==============================] - 0s 830us/step - loss: 0.1976 - accuracy: 0.9184\n",
      "Epoch 14/150\n",
      "59/59 [==============================] - 0s 898us/step - loss: 0.1960 - accuracy: 0.9198\n",
      "Epoch 15/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.1945 - accuracy: 0.9207\n",
      "Epoch 16/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1929 - accuracy: 0.9211\n",
      "Epoch 17/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1905 - accuracy: 0.9228\n",
      "Epoch 18/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1898 - accuracy: 0.9235\n",
      "Epoch 19/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1882 - accuracy: 0.9235\n",
      "Epoch 20/150\n",
      "59/59 [==============================] - 0s 847us/step - loss: 0.1873 - accuracy: 0.9240\n",
      "Epoch 21/150\n",
      "59/59 [==============================] - 0s 881us/step - loss: 0.1860 - accuracy: 0.9247\n",
      "Epoch 22/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1850 - accuracy: 0.9249\n",
      "Epoch 23/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1847 - accuracy: 0.9244\n",
      "Epoch 24/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.1833 - accuracy: 0.9249\n",
      "Epoch 25/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1823 - accuracy: 0.9250\n",
      "Epoch 26/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1809 - accuracy: 0.9262\n",
      "Epoch 27/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1805 - accuracy: 0.9257\n",
      "Epoch 28/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1804 - accuracy: 0.9260\n",
      "Epoch 29/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1784 - accuracy: 0.9264\n",
      "Epoch 30/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1775 - accuracy: 0.9267\n",
      "Epoch 31/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.1762 - accuracy: 0.9269\n",
      "Epoch 32/150\n",
      "59/59 [==============================] - 0s 847us/step - loss: 0.1756 - accuracy: 0.9275\n",
      "Epoch 33/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1747 - accuracy: 0.9275\n",
      "Epoch 34/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1738 - accuracy: 0.9278\n",
      "Epoch 35/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1734 - accuracy: 0.9282\n",
      "Epoch 36/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1721 - accuracy: 0.9285\n",
      "Epoch 37/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1707 - accuracy: 0.9291\n",
      "Epoch 38/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1713 - accuracy: 0.9290\n",
      "Epoch 39/150\n",
      "59/59 [==============================] - 0s 763us/step - loss: 0.1682 - accuracy: 0.9294\n",
      "Epoch 40/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1678 - accuracy: 0.9301\n",
      "Epoch 41/150\n",
      "59/59 [==============================] - 0s 763us/step - loss: 0.1671 - accuracy: 0.9309\n",
      "Epoch 42/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1667 - accuracy: 0.9310\n",
      "Epoch 43/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1652 - accuracy: 0.9309\n",
      "Epoch 44/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1641 - accuracy: 0.9316\n",
      "Epoch 45/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1640 - accuracy: 0.9319\n",
      "Epoch 46/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1633 - accuracy: 0.9323\n",
      "Epoch 47/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1626 - accuracy: 0.9328\n",
      "Epoch 48/150\n",
      "59/59 [==============================] - 0s 763us/step - loss: 0.1611 - accuracy: 0.9331\n",
      "Epoch 49/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1616 - accuracy: 0.9335\n",
      "Epoch 50/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1618 - accuracy: 0.9334\n",
      "Epoch 51/150\n",
      "59/59 [==============================] - 0s 847us/step - loss: 0.1603 - accuracy: 0.9340\n",
      "Epoch 52/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1591 - accuracy: 0.9346\n",
      "Epoch 53/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1589 - accuracy: 0.9348\n",
      "Epoch 54/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1572 - accuracy: 0.9351\n",
      "Epoch 55/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9349\n",
      "Epoch 56/150\n",
      "59/59 [==============================] - 0s 932us/step - loss: 0.1588 - accuracy: 0.9349\n",
      "Epoch 57/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.1559 - accuracy: 0.9362\n",
      "Epoch 58/150\n",
      "59/59 [==============================] - 0s 847us/step - loss: 0.1554 - accuracy: 0.9363\n",
      "Epoch 59/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1541 - accuracy: 0.9366\n",
      "Epoch 60/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1542 - accuracy: 0.9370\n",
      "Epoch 61/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1553 - accuracy: 0.9367\n",
      "Epoch 62/150\n",
      "59/59 [==============================] - 0s 847us/step - loss: 0.1537 - accuracy: 0.9373\n",
      "Epoch 63/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1534 - accuracy: 0.9377\n",
      "Epoch 64/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1525 - accuracy: 0.9380\n",
      "Epoch 65/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1533 - accuracy: 0.9374\n",
      "Epoch 66/150\n",
      "59/59 [==============================] - 0s 847us/step - loss: 0.1556 - accuracy: 0.9367\n",
      "Epoch 67/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1517 - accuracy: 0.9381\n",
      "Epoch 68/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1523 - accuracy: 0.9376\n",
      "Epoch 69/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1532 - accuracy: 0.9375\n",
      "Epoch 70/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1515 - accuracy: 0.9386\n",
      "Epoch 71/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1508 - accuracy: 0.9394\n",
      "Epoch 72/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1514 - accuracy: 0.9386\n",
      "Epoch 73/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1520 - accuracy: 0.9392\n",
      "Epoch 74/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1535 - accuracy: 0.9385\n",
      "Epoch 75/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1504 - accuracy: 0.9392\n",
      "Epoch 76/150\n",
      "59/59 [==============================] - 0s 763us/step - loss: 0.1490 - accuracy: 0.9398\n",
      "Epoch 77/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1498 - accuracy: 0.9396\n",
      "Epoch 78/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1494 - accuracy: 0.9389\n",
      "Epoch 79/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1496 - accuracy: 0.9398\n",
      "Epoch 80/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1483 - accuracy: 0.9404\n",
      "Epoch 81/150\n",
      "59/59 [==============================] - 0s 847us/step - loss: 0.1487 - accuracy: 0.9404\n",
      "Epoch 82/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.1496 - accuracy: 0.9406\n",
      "Epoch 83/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1511 - accuracy: 0.9395\n",
      "Epoch 84/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1476 - accuracy: 0.9405\n",
      "Epoch 85/150\n",
      "59/59 [==============================] - 0s 763us/step - loss: 0.1483 - accuracy: 0.9405\n",
      "Epoch 86/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.1488 - accuracy: 0.9401\n",
      "Epoch 87/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1475 - accuracy: 0.9406\n",
      "Epoch 88/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1476 - accuracy: 0.9413\n",
      "Epoch 89/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1476 - accuracy: 0.9406\n",
      "Epoch 90/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1470 - accuracy: 0.9412\n",
      "Epoch 91/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1485 - accuracy: 0.9406\n",
      "Epoch 92/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1466 - accuracy: 0.9411\n",
      "Epoch 93/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1456 - accuracy: 0.9418\n",
      "Epoch 94/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1463 - accuracy: 0.9415\n",
      "Epoch 95/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1462 - accuracy: 0.9415\n",
      "Epoch 96/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1457 - accuracy: 0.9417\n",
      "Epoch 97/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1458 - accuracy: 0.9411\n",
      "Epoch 98/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1455 - accuracy: 0.9414\n",
      "Epoch 99/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.1473 - accuracy: 0.9415\n",
      "Epoch 100/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1445 - accuracy: 0.9420\n",
      "Epoch 101/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1461 - accuracy: 0.9416\n",
      "Epoch 102/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1457 - accuracy: 0.9417\n",
      "Epoch 103/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1463 - accuracy: 0.9423\n",
      "Epoch 104/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1451 - accuracy: 0.9419\n",
      "Epoch 105/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.1452 - accuracy: 0.9420\n",
      "Epoch 106/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1466 - accuracy: 0.9416\n",
      "Epoch 107/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1461 - accuracy: 0.9416\n",
      "Epoch 108/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1466 - accuracy: 0.9416\n",
      "Epoch 109/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1442 - accuracy: 0.9420\n",
      "Epoch 110/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1445 - accuracy: 0.9423\n",
      "Epoch 111/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1455 - accuracy: 0.9420\n",
      "Epoch 112/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1455 - accuracy: 0.9419\n",
      "Epoch 113/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1446 - accuracy: 0.9423\n",
      "Epoch 114/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1442 - accuracy: 0.9428\n",
      "Epoch 115/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1452 - accuracy: 0.9421\n",
      "Epoch 116/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1446 - accuracy: 0.9419\n",
      "Epoch 117/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1447 - accuracy: 0.9423\n",
      "Epoch 118/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1440 - accuracy: 0.9424\n",
      "Epoch 119/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1471 - accuracy: 0.9408\n",
      "Epoch 120/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1455 - accuracy: 0.9426\n",
      "Epoch 121/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1434 - accuracy: 0.9420\n",
      "Epoch 122/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1432 - accuracy: 0.9422\n",
      "Epoch 123/150\n",
      "59/59 [==============================] - 0s 830us/step - loss: 0.1430 - accuracy: 0.9427\n",
      "Epoch 124/150\n",
      "59/59 [==============================] - 0s 763us/step - loss: 0.1430 - accuracy: 0.9425\n",
      "Epoch 125/150\n",
      "59/59 [==============================] - 0s 830us/step - loss: 0.1449 - accuracy: 0.9418\n",
      "Epoch 126/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1431 - accuracy: 0.9425\n",
      "Epoch 127/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1435 - accuracy: 0.9425\n",
      "Epoch 128/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1455 - accuracy: 0.9418\n",
      "Epoch 129/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.1437 - accuracy: 0.9421\n",
      "Epoch 130/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1446 - accuracy: 0.9425\n",
      "Epoch 131/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1457 - accuracy: 0.9411\n",
      "Epoch 132/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1428 - accuracy: 0.9426\n",
      "Epoch 133/150\n",
      "59/59 [==============================] - 0s 830us/step - loss: 0.1422 - accuracy: 0.9426\n",
      "Epoch 134/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1424 - accuracy: 0.9431\n",
      "Epoch 135/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1429 - accuracy: 0.9432\n",
      "Epoch 136/150\n",
      "59/59 [==============================] - 0s 830us/step - loss: 0.1419 - accuracy: 0.9424\n",
      "Epoch 137/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1438 - accuracy: 0.9421\n",
      "Epoch 138/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1450 - accuracy: 0.9417\n",
      "Epoch 139/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1426 - accuracy: 0.9430\n",
      "Epoch 140/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1419 - accuracy: 0.9425\n",
      "Epoch 141/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1440 - accuracy: 0.9417\n",
      "Epoch 142/150\n",
      "59/59 [==============================] - 0s 864us/step - loss: 0.1419 - accuracy: 0.9425\n",
      "Epoch 143/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1429 - accuracy: 0.9431\n",
      "Epoch 144/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1413 - accuracy: 0.9433\n",
      "Epoch 145/150\n",
      "59/59 [==============================] - 0s 780us/step - loss: 0.1420 - accuracy: 0.9427\n",
      "Epoch 146/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1426 - accuracy: 0.9422\n",
      "Epoch 147/150\n",
      "59/59 [==============================] - 0s 763us/step - loss: 0.1426 - accuracy: 0.9425\n",
      "Epoch 148/150\n",
      "59/59 [==============================] - 0s 814us/step - loss: 0.1416 - accuracy: 0.9425\n",
      "Epoch 149/150\n",
      "59/59 [==============================] - 0s 797us/step - loss: 0.1411 - accuracy: 0.9432\n",
      "Epoch 150/150\n",
      "59/59 [==============================] - 0s 831us/step - loss: 0.1410 - accuracy: 0.9430\n"
     ]
    }
   ],
   "source": [
    "adam = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=1024, epochs=150) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the loss curve using the same function as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEJCAYAAACXCJy4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjFElEQVR4nO3deZhkdX3v8fc51VW99yxNjzMsA0GdryuMMMhNCEquJPeGa6I8IkRQQQLER3jUKBIV3IP68Gg0ymIeDMEbgsYLgXu9gCuGJS7IVRYVvyFERmEGpmkGpvda7x/nVE9VdXV3dXfNVHHq83qefqbO75xT/enqnu/5nd/ZglKphIiIJF/Y6gAiIrJ/qOCLiHQIFXwRkQ6hgi8i0iFU8EVEOoQKvohIh1DBl+c8MzvMzCZanUOk3angi4h0iK5WBxDZl8xsDXAFsBUoAbcBH3T3vJl9DDgZyAJjwFnuvnOh9pr3HQC+CBwH5IGbgYuBfwB+7u6fiZe7tjxtZo8CPwaOAD4KXOLuL4+XWwv8Gjgc6AMuBzYDaeBr7v7J5n4y0onUw5ek+wJR0X45sA04ErjQzA4B3g0c4+7bgG8Dxy7UXud9Pw70AC8m2pgcB7y6gTw/d/cXA18HBsxsW9z+JuAWd98N/CNwjbsfDbwSONHMTl3mzy0yjwq+JN0fA5e7e8ndZ4EvxW2PA/cDPzWzzwD3ufvNi7TXOhH4e3cvuHvW3V/t7v/aQJ67ANy9BFwDnBW3vw242sz6iTYcnzCz+4AfEfX0ty7z5xaZRwVfki4kGsqpnE67e5GosJ5FtAfwOTO7bKH2Ou+br3xfMzvEzIbjtqBiuUzNepUHl68B3mhmW4G17n4HkIrX/z133+ruW4H/AmhIR1ZNBV+S7lvABWYWmFk3cB7wHTM7Evg58JC7fwr4HHDMQu113ve7wJlmFsbvewPRhmKUaOgIMzuQRYZ53P1x4B7g74Avx217iHr174nfYy3wb8DrVvMhiIAO2kpy9Nc5NfN3gXcSHVx9kKi3/U3gUnfPmtnXgXvj9aaBd7r7/fXa63y/jwF/SzT8kwL+2d3/xcx+AvyTmTnwKHD7ErmvJtpY/GlF2+nA5WZWzvxVd/+nhj4FkUUEuj2yiEhn0JCOiEiHUMEXEekQKvgiIh1CBV9EpEO061k63USnwu0ECi3OIiLyXJECNgE/AWZrZ7ZrwT+G+IpEERFZtuOBu2sb27Xg7wTYvXuSYnH5p40ODw8wNtbed8tVxuZQxuZQxtVrh3xhGLBuXT/ENbRWuxb8AkCxWFpRwS+v2+6UsTmUsTmUcfXaKF/doXAdtBUR6RAq+CIiHUIFX0SkQ6jgi4h0CBV8EZEOoYIvItIhElfwf/PkOH9+6XeYmM61OoqISFtJXMHftXuaXU9P8cz4vKuKRUQ6WkMXXpnZR4BT48lb3P2imvlbiR7RNgTcCbzd3fNmthm4DtgAOHCGu+/TS9GCIHqcaFEPdhERqbJkD9/MTgT+CHgFsBU42sxOrlnsOuACd99C9ADmc+P2K4Er3f1FwL3Ah5qUe0Fh/BOp4IuIVGtkSGcn8F53z7p7DngI2FyeaWaHAr3u/qO46VrgjWaWBl5F9LzOufYm5V5QWO7hF/f1dxIReW5ZckjH3X9Rfm1mLyQa2jmuYpEDqb5Rz07gYOAAYI+752vaGzY8PLCcxQFYNzYFwJo1vYyMDC57/f2p3fOBMjaLMjZHu2ds93wN3zzNzF4K3AK8z90frpgVApXjJwFQrNNO3N6wsbGJZd+MaHzPTLTu05OM9qeXte7+NDIyyOjoeKtjLEoZm0MZm6PdM7ZDvjAMFu0oN3SWjpkdB3wPeL+7f6Vm9mNEN9wv2wjsAHYBa8wsFbdvitv3qTAsD+loDF9EpFIjB20PAW4GTnf3r9XOd/ftwEy8UQB4C3BbPN5/F3Ba3P5W4LZmhF5MXO910FZEpEYjQzoXAj3A35hZue1LwJ8CH3b3e4EzgKvNbAj4KfCFeLl3AF8xs0uA3wBvamL2uuZ6+Cr4IiJVGjlo+y7gXXVmfalimfuBV9ZZdztwwiryLZvO0hERqS9xV9qqhy8iUl/yCn7cwy/poK2ISJXEFfxAB21FROpKXMHfO6TT4iAiIm0meQU/0Hn4IiL1JK/g66CtiEhdySv45TF89fBFRKoksOCrhy8iUk/yCn7cxVe9FxGplriCH+igrYhIXYkr+DpoKyJSX/IKvg7aiojUlbyCrwuvRETqSl7B1xi+iEhdiS34JY3hi4hUSV7Bj38iHbQVEamWuIKv0zJFROpr5BGHxI8u/AHwWnd/tKJ9K3BtxaIjwG53f5mZnQl8GngynneLu1/chMyL0kFbEZH6liz4ZnYscDWwpXaeu98HbI2X6wPuAd4ez94GvMfdv9qkrA3RQVsRkfoaGdI5Fzgf2LHEch8A7nD3u+PpY4AzzexBM7vOzNatIueyhGGgMXwRkRqNPMT8HAAzW3AZM1sDnAe8vKJ5J/AZoqGgTwKXA2csJ9zw8MByFp8TBgE9PWlGRgZXtP7+0u75QBmbRRmbo90ztnu+hsbwG/Bm4GZ331VucPeTy6/N7DLgkeW+6djYxIqGZsIwYGJyltHR8WWvu7+MjAy2dT5QxmZRxuZo94ztkC8Mg0U7ys06S+f1wNfKE2a2xsz+smJ+AOSb9L2WlAqhWNxf301E5Llh1QXfzALgaOCHFc0TwEXxAV+AC4CbVvu9GhWGocbwRURqrKjgm9mtZrYtnhwBsu4+U57v7gXgVOAqM3uIaINw0WrDNioMdNBWRKRWw2P47n5YxeuTKl7vAjbWWf4u4KhV5luRVBhQ0mmZIiJVEnelLUS3V1APX0SkWjILfhDooK2ISI1kFnxdeCUiMo8KvohIh0hmwQ8C3UtHRKRGMgt+GOhumSIiNRJb8HVapohItWQWfF14JSIyTzILfqgxfBGRWskt+Kr3IiJVElnwUxrSERGZJ5EFX0M6IiLzJbbgl9TDFxGpksyCrwuvRETmSWbBD9FBWxGRGsks+DpoKyIyTzILvg7aiojM09ATr8xsCPgB8Fp3f7Rm3keAs4HdcdPV7n6FmW0GrgM2AA6c4e4TzQq+GN0tU0RkviV7+PGDyO8GtiywyDbgz9x9a/x1Rdx+JXClu78IuBf4UDMCN0IPQBERma+RHv65wPnAPy4wfxvwQTM7FLgTuBAoAK8CXh8vcy1wB/BXq8jaMJ2WKSIy35IF393PATCzefPMbAD4GfA+4D+ICvuHgMuBPe6ejxfdCRy83HDDwwPLXQWICj5BwMjI4IrW31/aPR8oY7MoY3O0e8Z2z9fQGP5C4jH5k8rTZvZZ4Bqi4ZzaLvayB1nGxiZWdPA1FQTk8gVGR8eXve7+MjIy2Nb5QBmbRRmbo90ztkO+MAwW7Siv6iwdM9tsZmdXNAVADtgFrDGzVNy+Cdixmu+1HDpLR0RkvtWeljkNXGZmv2NmAdFY/03ungPuAk6Ll3srcNsqv1fDNIYvIjLfigq+md1qZtvcfRT4C+AbRKdeBsBn48XeAZxnZr8EjgcuaULehqR0e2QRkXkaHsN398MqXp9U8fpG4MY6y28HTlhdvJXRkI6IyHzJvNJWt1YQEZknmQVfPXwRkXmSW/BV70VEqiSz4GtIR0RknmQW/DCgpC6+iEiVZBb8APXwRURqJLPgh7pbpohIreQWfPXwRUSqJLLgp4IA0LCOiEilRBb8MIwLvg7ciojMSXTB1w3URET2SmbBLw/p6MCtiMicZBb8UGP4IiK1VPBFRDpEMgt+oIO2IiK1klnw53r4LQ4iItJGkl3wVfFFROY09MQrMxsCfgC81t0frZn3OuBjRI83/DXwNnffbWZnAp8GnowXvcXdL25W8MWUh3R0WqaIyF5LFnwzOxa4GthSZ94QcBVwjLs/bmYfBz4KvAvYBrzH3b/a1MQNSMX7Lerhi4js1ciQzrnA+cCOOvPSwPnu/ng8/QCwOX59DHCmmT1oZteZ2bpVp22QztIREZlvyR6+u58DYGb15o0BN8Xze4H3A1+MZ+8EPkM0FPRJ4HLgjOWEGx4eWM7ic8LfPAPAmrV9jIwMrug99od2zlamjM2hjM3R7hnbPV9DY/hLMbM1RIX/fnf/CoC7n1wx/zLgkeW+79jYxIqGZco9/KfGJukOlr36fjEyMsjo6HirYyxKGZtDGZuj3TO2Q74wDBbtKK/6LB0z2wTcRTScU94bWGNmf1mxWADkV/u9GjV3Lx2N4YuIzFlVwTezFPAN4Ovu/m53L1fYCeCi+IAvwAXEQz/7Q6jbI4uIzLOiIR0zuxX4MHAIcBTQZWanxLPvdfdzzOxU4Kp4bP/fgbc2I3AjUjpoKyIyT8MF390Pq3h9UvzyXhbYS3D3u4g2Bvvd3guvWvHdRUTaU7KvtFUPX0RkTjILvm6eJiIyTzILvp54JSIyT6ILvnr4IiJ7JbPgB7o9sohIrUQWfJ2WKSIyXyILvoZ0RETmS2bB15W2IiLzJLPg68IrEZF5El3wdVqmiMheySz48S2RNaQjIrJXMgu+DtqKiMyT7IKvHr6IyJxkFvxAB21FRGols+Crhy8iMk8iC76utBURma+hB6CY2RDwA+C17v5ozbytwJeBIeBO4O3unjezzcB1wAbAgTPcfaJ50RdWHtLRM21FRPZasocfP5f2bmDLAotcB1zg7luIHlZ+btx+JXClu7+I6MlYH1p93MbsHdLZX99RRKT9NTKkcy5wPrCjdoaZHQr0uvuP4qZrgTeaWRp4FXBDZftqwzZKp2WKiMy35JCOu58DYGb1Zh8I7KyY3gkcDBwA7HH3fE37flEe0imo4IuIzGn4IeYLCIHKqhoAxTrtxO3LMjw8sKJQ07PRdqavL8PIyOCK3mN/aOdsZcrYHMrYHO2esd3zrbbgPwZsqpjeSDT0swtYY2Ypdy/Ey8wbElrK2NjEioZlhtb2ATA+McPo6Piy198fRkYG2zZbmTI2hzI2R7tnbId8YRgs2lFe1WmZ7r4dmDGz4+KmtwC3uXsOuAs4LW5/K3Dbar7Xcugh5iIi862o4JvZrWa2LZ48A/icmf0KGAC+ELe/AzjPzH4JHA9cstqwjdJZOiIi8zU8pOPuh1W8Pqni9f3AK+ssvx04YXXxVmbubpmq+CIicxJ5pW0QBIRBoCttRUQqJLLgA4Shbq0gIlIpuQU/CCjpbpkiInMSW/CDUEM6IiKVElvwwyDQQVsRkQoJLvgawxcRqZTcgh8GOg9fRKRCcgu+hnRERKokt+DroK2ISJXkFvxAT7wSEamU2IIf6EpbEZEqiS34OmgrIlItuQVfB21FRKokt+DroK2ISJXkFvxAt0cWEamU4IIfoA6+iMheiS34unmaiEi1hp54ZWanEz2iMA183t2vqJi3Fbi2YvERYLe7v8zMzgQ+DTwZz7vF3S9uQu4l6aCtiEi1JQu+mR0EXAocDcwCPzCz77v7LwHc/T5ga7xsH3AP8PZ49W3Ae9z9q01PvgQ9AEVEpFojQzonAre7+9PuPgncAJyywLIfAO5w97vj6WOAM83sQTO7zszWrT5yY9TDFxGp1kjBPxDYWTG9Ezi4diEzWwOcB3ysZtlPAEcAvwUuX3HSZVLBFxGp1sgYfghUVs4AqPfwwDcDN7v7rnKDu59cfm1mlwGPLCfc8PDAchav0t3dRS5fZGRkcMXvsa+1c7YyZWwOZWyOds/Y7vkaKfiPAcdXTG8EdtRZ7vXAJ8sTcY//bHf/XNwUAPnlhBsbm1hRL31kZJBCvsBstsDo6Piy198fRkYG2zZbmTI2hzI2R7tnbId8YRgs2lFuZEjnu8BrzGwkPij7BuCblQuYWUB0UPeHFc0TwEVmdmw8fQFw0zKyr0oQakhHRKTSkgXf3R8HLga+D9wHXO/u95jZrWa2LV5sBMi6+0zFegXgVOAqM3uIaINwUZPzLyjU3TJFRKo0dB6+u18PXF/TdlLF611EQz21690FHLXKjCsSHbRtxXcWEWlPib3SNhUGlNTDFxGZk9iCr1sriIhUS2zB190yRUSqJbfgq4cvIlIluQVfB21FRKoku+Crhy8iMie5BV93yxQRqZLcgh8ElHTQVkRkTmILfnRaZqtTiIi0j8QWfN0eWUSkWrILvsbwRUTmJLfg66CtiEiV5BZ8nYcvIlIluQVfN08TEamS2IIf6KCtiEiVxBb8MIgexKtevohIJLkFPwwAHbgVESlr6IlXZnY6cAmQBj7v7lfUzP8IcDawO2662t2vMLPNwHXABsCBM9x9olnhFxMGccEvQiqxmzURkcYtWQrN7CDgUuD3ga3AeWb2kprFtgF/5u5b46/yBuFK4Ep3fxFwL/ChpiVfgnr4IiLVGunhnwjc7u5PA5jZDcApwMcrltkGfNDMDgXuBC4ECsCrgNfHy1wL3AH8VTOCL2VvD18FX0QEGhvDPxDYWTG9Ezi4PGFmA8DPgPcRPbB8LVFP/gBgj7vn6623r8UdfPXwRURijfTwQ6ITXsoCYO6SpnhM/qTytJl9FriGaDinttou61Ko4eGB5SxeZXCoB4B16/pZM9C94vfZl0ZGBlsdYUnK2BzK2BztnrHd8zVS8B8Djq+Y3gjsKE/EB2ZPdPdr4qYAyAG7gDVmlnL3ArCpcr1GjI1NrGhIZmRkkKnJLACjT02Qnc4u+z32tZGRQUZHx1sdY1HK2BzK2BztnrEd8oVhsGhHuZEhne8CrzGzETPrA94AfLNi/jRwmZn9jpkFwPnATe6eA+4CTouXeytw2wp+hhWZO2irMXwREaCBgu/ujwMXA98H7gOud/d7zOxWM9vm7qPAXwDfIDr1MgA+G6/+DqKzen5JtJdwSfN/hPrKY/i68EpEJNLQefjufj1wfU3bSRWvbwRurLPeduCE1UVcGZ2lIyJSLbGXJOk8fBGRah1Q8FscRESkTSS34GtIR0SkSnILvoZ0RESqJLfgl6+0VQ9fRARIdMGPKr46+CIikcQW/CDu4uf1YFsRESDBBf9563oB2P5E+16KLSKyPyW24G9c38eGtb088MhYq6OIiLSFxBb8IAg44gXDPLR9N7O5QqvjiIi0XGILPsCRzz+AXL7IQ9t3L72wiEjCJbrgbzlkLd2ZlIZ1RERIeMFPd4W89LD13P8fT+mumSLS8RJd8AGOfP4wu8dneeTxPa2OIiLSUokv+EfbBtYOZLj2m78il9fBWxHpXIkv+H09XZx90ovZ8dQkN97xn62OIyLSMokv+AAvO3yYPzjqIL7zk9/qAK6IdKyGnnhlZqcTPZ4wDXze3a+omf864GNEjzf8NfA2d99tZmcCnwaejBe9xd0vblb45Tj1D17AI489y5f+98/54FuO5uCRhR/0KyKSREv28M3sIOBS4PeBrUTPqH1Jxfwh4Crgf7j7kcADwEfj2duA97j71virJcUeoDud4p2nHEF3JsXf/q8HeHZitlVRRERaopEhnROB2939aXefBG4ATqmYnwbOjx92DlHB3xy/PgY408weNLPrzGxds4KvxPqhHt51yhGMT2f5zNfuY89UtpVxRET2q0YK/oHAzorpncDB5Ql3H3P3mwDMrBd4P3BzxbKfAI4AfgtcvvrIq3PYxiHedcqR7Hpmms9+7T7GVfRFpEM0MoYfApVXLQXAvHsOm9ka4Cbgfnf/CoC7n1wx/zLgkeWEGx5e+Tj7yMjgovMGBnv462t+zKf+6adccvaxHLpxaMXfa6UWy9gulLE5lLE52j1ju+drpOA/BhxfMb0R2FG5gJltAr4F3A78Zdy2Bjjb3T8XLxYA+eWEGxubWNETq0ZGBhkdXfy2yIes7+WiN72CL/7Lg1z4t3dy6n99AccfsYlUuH9OXGokY6spY3MoY3O0e8Z2yBeGwaId5Uaq23eB15jZiJn1AW8AvlmeaWYp4BvA19393e5ertATwEVmdmw8fQHRHkDbeP5Ba/jwmds4ZMMA//Obzof//h7ufmCn7q4pIom0ZA/f3R83s4uB7wMZ4Mvufo+Z3Qp8GDgEOAroMrPywdx73f0cMzsVuCoe2/934K375KdYhfVDPbz/jKP42cNPcdOd/8k1tz7EV7/3MFtfcABHPH+Y5x84xPo1PXOPTBQRea5q6Dx8d78euL6m7aT45b0ssKfg7ncRbQzaWhAEHLVlhFe88AAefuxZ7rx/Bw88MsYPf/EEAJmukIM3DHD4piE2HdDP2v4Mawe7WTvQzWBfmq5UR1y/JiLPcQ0V/E4RBAFbDlnLlkPWUiyWePSJcX67a5wdT02x/Yk93PnADrK5+c/I7c6kGOhJM9Cbpr+3K/43TX+5rSdqK7cP9KZZv4JjEyIiq6GCv4AwDDj8wCEOP3Dv2TvFYolnJ7M8MzHLsxPRv+NTWSZn8kxM55iYzjE5k2NszyyT8evF7sqcSYf0pFN0Z1J0p7voyUSvy21V0+kUQRAQBDDUn2FNf4be7i4y8bxMOiTTlaIrFRBo+ElE6lDBX4YwDFg32M26we6Gli+WSkzPRhuDyenyvzkmZnIEYcjTz0wzkyswm80zky0wky0wPZvnmfFZZrIFZnNRW74wf69iwYxBEBX/dIpMV0h3JkWmK0V3uS2dorsrrNpQDPVn2LS+j7WD3XSlwvgroH8mR75QJBVqIyKSBCr4+1AYBPT3REM71FxjvJxTuPKFItlcgRJQKJYYn8zyzGSW2XijkM0VyOaKZPPl6Wj58uvZfIFstsCzk9m5ZWdzBbL5Qt0hqno/R19PF309XfT3dNGT6aIrFZIKA7pSAalUSDoV0tUVbSjSXdF0KhVGD54pQW93F/29XfT1RENcqTAkDJkb9gKizPkiuVyBwf4Mg71pbWhEmkgF/zmg3OsuG+rLcNBIc967WCrx7ESWHWOTjE9myRdK5AtFcoUiPT0ZntkzTTZXYGomz+RMjqmZPNPZPNOzeQrFEoViiXy+SL5YJJcvRuvmS8vaK1lIb3e0gSlvQKINSkg6FVAolpjOFuaGvHoyqehMqgACAsIACGA2V2T3nhnyxRKbNwywYV0vuXyRbD7eiJZgsC/NYF+GTFdIKhWQzRXn9rBmcwVmswVyhSLDQz1sXN/HUH+G/p4uCsUS2VyBIAgIg4AwDEiF0b9hGGVIhQFT+RKP7XyWqdk80zN5enu6OHikn8HeDLnC3s+tvGHNpEOKxRLFYvT7KRRLFEuluK1EEAQM9HaR7kqt/g9AOooKfocLg4WHqVZzIUkpLlTl01mnZvduMCZnchTjjcXEdI6JqRxhGMwNQ6W7Qp6dyPLE7immZ/PRBqVQIlcoks9HxTgVBqzpz5BKhYw9M81svAdUKpUoleJ/gXQqZP1QD90B/Ozhp5iYzgHR4y8zXSFBEDA5nWOhQy3lYbEwDNgzkV1wuVbIpEMGetP0dndRKkV7f6V44xAEzG2Eog1StE50nCnPmv4MB6zpIVcoMjGdJwwCiqUSYUC8sQqq3mM2V2ByOkexBOlUUDH0F9LVFczt0QUBcQegRKFQpFSKPuvo804RBDCTLZDLR0OFqfi9otfRhu6Jp6cY2zNDdzpFf7xn2dedZv2aXsKgFA0xEuUDGJ/KMT6VpTuTYqg/QxhEHYLxqeg4WyoMWdOfmfve0VeennSK58Ub8PJnFIYBlCBfLAIBPZkUQfy5jcd/q/likXUD0Vl6PZkUBPDsRJZ8CbpTAYN9GXL5qJPQ1x2dyFEqQaEQ/R0XilGHqFAs0ZUK5oZX013x5xAGvOzwYbrTzd+gq+DLPhEE0XBPWfkspWZbzkapVCqRzRVJp8Oq6yoKxSKTM3myuQKFQolMvMfQnY4KfVk2V2DXM9OMT+WYmsmRCkMy6WjPqxgX2kLcCy/3yguFEiPDA+SyOfq6u+jt7mJ8Kstjo5PMzOajPZZ4z6VYKjE1kyebL+zdU6jYeyjvQRSKpehYUPw1PZuv2rMICCgR5SiVmNs7ADhs0xADPWmemZhl9NlpMl0pNm/ooa83w/RMNv45os+q/LpYKrF2oJu+TV2kwmCuoOfz0Z5gPi5k2dk8pVIp3gsL6c10EQSQy0dDiBPTOYpF6OmOiluxWIo2JDN5CoUShWK0V/i8dX285LB1ZHNFpmZyTM3m2TOVZfTZacYns3P5SgAlGOjtYqg/w9ieGX61fTelEqRSAQO9adYOdFMolvjNrgmAub3BdQPdTM/muf8/nmJyJh99VhV/K2EQ7P0eQFf8fgO9GbpSAb95coI9k3vvw5VJhwz1ZXhmYpZ8IVorCFj0pI3FnPnfjVdvPWhlKy9CBV86RhAEdGfm95pSYfSfdSmZdGpFz1Go3ShtXN/HCw9eu+z32Zfa4bYAS9nXGct7hxD19EulEtl8kVKpNHeWXKViqUQuV6RYKtGTSbFhwxBP7trDzGyBTDrqrU/PFpiayc1trFNzx76if/OFvcOLs7kCxRKEQfQ3si+o4IuIwNxpz5XTiw2rhHU6EOXjMGXlkx0WkgmjM+fYB3u/9egSURGRDqGCLyLSIVTwRUQ6hAq+iEiHUMEXEekQKvgiIh2iXU/LTAFVF70s12rW3V+UsTmUsTmUcfVana/i+9c9nzQorfRSsH3r94G7Wh1CROQ56njg7trGdi343cAxwE5AD5gVEWlMCtgE/ASYrZ3ZrgVfRESaTAdtRUQ6hAq+iEiHUMEXEekQKvgiIh1CBV9EpEOo4IuIdAgVfBGRDtGut1ZYMTM7HbgESAOfd/crWhwJM/sIcGo8eYu7X2RmJwJ/A/QC/+zul7QsYAUz+wxwgLuf1W4ZzexPgI8A/cC33f1dbZjxzcAH4snb3P3CdsloZkPAD4DXuvujC+Uys63Al4Eh4E7g7e6eb1HG84B3AiXgXuAv3D3bqoy1+SraLwBOcfcT4umW5FtKonr4ZnYQcCnRrRm2AueZ2UtanOlE4I+AV8SZjjazNwHXAK8DXgwcY2Z/3LKQMTN7DXBm/LqXNspoZocDXwJeDxwBHBXnaaeMfcAXgFcDRwLHxxuplmc0s2OJLrXfEk8v9vu9DrjA3bcAAXBuizJuAd4H/B7R7zwEzm9Vxtp8Fe0vAd5fs3hLPsOlJKrgAycCt7v70+4+CdwAnNLiTDuB97p71t1zwENEfzAPu/uv463+dcAbWxnSzNYTbSw/GTe9kvbKeDJRL/Sx+HM8DZhqs4wpov9T/UR7mGlgD+2R8VyiYrkjnq77+zWzQ4Fed/9RvNy17L+8tRlngXe4+x53LwEPAptbmLE2H2bWDfwd8OGKtlZ+hotK2pDOgUQFtmwn0R92y7j7L8qvzeyFREM7X2R+zoP3c7RafwdcDBwST9f7LFuZ8QVA1sz+D7AZ+L/AL2ijjO4+bmYfAn5FtDG6gzb5HN39HAAzKzctlKtleWszuvt2YHvcNgJcAJzVqox1PkOATxHtKf26oq0tfuf1JK2HHxKN9ZUFQLFFWaqY2UuB7xDtov4nbZTTzM4Bfuvu36tobrfPsotoD+7Pgd8FjgUOp40ymtkRwNnAoUT/6QtEe3Ntk7HCQr/fdvu9l4dqvwf8vbv/K22S0cz+ENjs7v9QM6st8tWTtB7+Y0S3BS3bSMXuV6uY2XHAjcC73f1rZvZqojvalbU652nAJjO7D1gPDBAVrco7lbY64xPAd919FMDMbiLaTW6njP8N+J677wIws2uBC2mvjGWPUf9vcKH2ljCzFwHfAr7g7p+Nm9sl45uAl8b/bwaAjWb2z8BFtEe+eZJW8L8LfDTe/ZsE3gCc18pAZnYIcDNwmrvfHjf/OJplLyDaFTydaLewJdz9D8uvzews4ATg7cDD7ZKRaAjnK2a2FhgH/pjoGM372yjj/cBlZtZPNKTzJ0S/6zPaKGNZ3b9Bd99uZjNmdpy7/xvwFuC2VgQ0s0Hg28DF7v6P5fZ2yejuZ1dkPQH4qLufFk+3PF89iRrScffHicahvw/cB1zv7ve0NFTUw+sB/sbM7ot7A2fFXzcCvyQa872hRfnqcvcZ2iiju/8YuIzoLIlfEo3tXkV7Zfw28FXg/wEPEB20/ShtlLFsid/vGcDnzOxXRD3XL7QiI3AO8DzgveX/O2b28TbLuJC2zKf74YuIdIhE9fBFRGRhKvgiIh1CBV9EpEOo4IuIdAgVfBGRDqGCLyLSIVTwRUQ6hAq+iEiH+P9wy4w8bC6bggAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_costs(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can also be evaluated on the test set with familiar code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9420\n",
      "F1: 0.6591\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.array(y_pred > 0.5, dtype=int).squeeze()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"F1: %.4f\" % f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
